{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lqcg7E5_UEr",
        "outputId": "e41dd54d-1a95-43e3-bebf-848ac319a0af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.352-py3-none-any.whl (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.4/794.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.6.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.6-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain)\n",
            "  Downloading langchain_core-0.1.3-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain)\n",
            "  Downloading langsmith-0.0.74-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typing-extensions, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.3 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.352 langchain-community-0.0.6 langchain-core-0.1.3 langsmith-0.0.74 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.6.0 tiktoken-0.5.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C-pPrC6wOayV"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import LLMChain, ConversationChain\n",
        "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
        "                                                  ConversationSummaryMemory,\n",
        "                                                  ConversationBufferWindowMemory,\n",
        "                                                  ConversationKGMemory)\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ApzoD-YAARv"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCba-mbeA8Ia",
        "outputId": "10880a55-46aa-4e42-c188-0a81677a34bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: hi!\\nAI: what's up?\"}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H8-ipRNLA_k3"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J71oaOOdBFiZ",
        "outputId": "eb4b5713-c5f9-4d9f-c541-d26690a8af7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'chat_history': \"Human: hi!\\nAI: what's up?\"}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8BApVU10BLxJ"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp4ddPa1Bh6K",
        "outputId": "4e3a5f7e-51ad-4184-eb92-1c32d87af179"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='hi!'),\n",
              "  AIMessage(content=\"what's up?\")]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "czoO1rn_EzCD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your api key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ccIIvNG7BuvP"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "# Notice that \"chat_history\" is present in the prompt template\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "New human question: {question}\n",
        "Response:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "# Notice that we need to align the `memory_key`\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlyjzIqcELDm",
        "outputId": "94a84f81-3368-4734-d82a-fda8da83ee13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
            "\n",
            "Previous conversation:\n",
            "\n",
            "\n",
            "New human question: hi\n",
            "Response:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': '',\n",
              " 'text': ' Hello! How are you doing today?'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pODaI1QOFVyW"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a nice chatbot having a conversation with a human.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzWjk5wBL2I1",
        "outputId": "d36dbbc3-18a3-4f97-a318-cb23915330cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
            "Human: hi\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='Hello! How can I assist you today?')],\n",
              " 'text': 'Hello! How can I assist you today?'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fwtLjaH9L-Ib"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EAhlaeGvMpsb"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Z4Mdr9NImj",
        "outputId": "caa073b0-b3b0-420a-e9c9-1176ceb57446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, Any],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        response = self.generate([inputs], run_manager=run_manager)\n",
            "        return self.create_outputs(response)[0]\n",
            "     def apply(\n",
            "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
            "    ) -> List[Dict[str, str]]:\n",
            "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
            "        callback_manager = CallbackManager.configure(\n",
            "            callbacks, self.callbacks, self.verbose\n",
            "        )\n",
            "        run_manager = callback_manager.on_chain_start(\n",
            "            dumpd(self),\n",
            "            {\"input_list\": input_list},\n",
            "        )\n",
            "        try:\n",
            "            response = self.generate(input_list, run_manager=run_manager)\n",
            "        except BaseException as e:\n",
            "            run_manager.on_chain_error(e)\n",
            "            raise e\n",
            "        outputs = self.create_outputs(response)\n",
            "        run_manager.on_chain_end({\"outputs\": outputs})\n",
            "        return outputs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWNt-YUIPnJz",
        "outputId": "f914381a-2343-46ea-fc59-793519bfe27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, Any],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        response = self.generate([inputs], run_manager=run_manager)\n",
            "        return self.create_outputs(response)[0]\n",
            "     def apply(\n",
            "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
            "    ) -> List[Dict[str, str]]:\n",
            "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
            "        callback_manager = CallbackManager.configure(\n",
            "            callbacks, self.callbacks, self.verbose\n",
            "        )\n",
            "        run_manager = callback_manager.on_chain_start(\n",
            "            dumpd(self),\n",
            "            {\"input_list\": input_list},\n",
            "        )\n",
            "        try:\n",
            "            response = self.generate(input_list, run_manager=run_manager)\n",
            "        except BaseException as e:\n",
            "            run_manager.on_chain_error(e)\n",
            "            raise e\n",
            "        outputs = self.create_outputs(response)\n",
            "        run_manager.on_chain_end({\"outputs\": outputs})\n",
            "        return outputs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cXjkeMGDPuHq"
      },
      "outputs": [],
      "source": [
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rGYD8CWPz5o",
        "outputId": "3ba07056-0ded-4105-f842-3eedb3b10b08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Good morning AI!',\n",
              " 'history': '',\n",
              " 'response': 'Good morning! How can I assist you today?'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_buf(\"Good morning AI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQtq3bJWP3Dp",
        "outputId": "c733dcb7-d5af-47d1-a47a-e547b666c66b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Can you please tell me your name AI?',\n",
              " 'history': 'Human: Good morning AI!\\nAI: Good morning! How can I assist you today?',\n",
              " 'response': 'Of course! My name is OpenAI. How can I assist you today?'}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_buf(\"Can you please tell me your name AI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "61eaZyCrQbdN"
      },
      "outputs": [],
      "source": [
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "K03BBV5RQuXa",
        "outputId": "4843d814-d224-47e1-a47c-af714ad9cfe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 271 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'That sounds like a fascinating concept! The idea of a Big Hairy Audacious Goal (BHAG) was first introduced by James Collins and Jerry Porras in their book \"Built to Last: Successful Habits of Visionary Companies.\" They define a BHAG as a long-term goal that is both ambitious and inspiring, capable of energizing and aligning the efforts of employees and stakeholders. The purpose of a BHAG is to push an organization beyond its current capabilities and create a sense of purpose and direction. It\\'s a powerful tool for motivating and guiding the actions of individuals and teams towards a common objective.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,\\\n",
        "     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "8H_3KziPQ1nR",
        "outputId": "4382d2da-f93e-4a4f-cde2-d47a31cf017d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 483 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Absolutely! The BHAG can indeed help an organization focus on a long-term goal by providing a clear and compelling vision for the future. It serves as a guiding star that helps leaders make strategic decisions and align resources towards achieving that goal. By setting a BHAG, organizations create a sense of urgency as they understand the importance of taking action and making progress towards their ambitious goal. This sense of urgency can drive innovation, collaboration, and a relentless pursuit of excellence. Moreover, a BHAG can attract and retain top talent by offering a compelling vision and a sense of purpose. It acts as a magnet for individuals who are motivated by challenging and meaningful work. By embracing their BHAG, organizations can galvanize their employees and stakeholders, fostering a strong and unified culture where everyone is motivated and aligned towards achieving success.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fturnCyNREeJ",
        "outputId": "24b4e096-c1d5-4f45-86cd-4b5da6d8eae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 510 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This book introduces the concept of a Big Hairy Audacious Goal (BHAG).'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"What concept this book introduces?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIe6VI0DRVW4",
        "outputId": "8c8c3eff-228c-42ec-c14b-f8743b8331ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Good morning AI!\n",
            "AI: Good morning! How can I assist you today?\n",
            "Human: Can you please tell me your name AI?\n",
            "AI: Of course! My name is OpenAI. How can I assist you today?\n",
            "Human: This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\n",
            "AI: That sounds like a fascinating concept! The idea of a Big Hairy Audacious Goal (BHAG) was first introduced by James Collins and Jerry Porras in their book \"Built to Last: Successful Habits of Visionary Companies.\" They define a BHAG as a long-term goal that is both ambitious and inspiring, capable of energizing and aligning the efforts of employees and stakeholders. The purpose of a BHAG is to push an organization beyond its current capabilities and create a sense of purpose and direction. It's a powerful tool for motivating and guiding the actions of individuals and teams towards a common objective.\n",
            "Human: The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\n",
            "AI: Absolutely! The BHAG can indeed help an organization focus on a long-term goal by providing a clear and compelling vision for the future. It serves as a guiding star that helps leaders make strategic decisions and align resources towards achieving that goal. By setting a BHAG, organizations create a sense of urgency as they understand the importance of taking action and making progress towards their ambitious goal. This sense of urgency can drive innovation, collaboration, and a relentless pursuit of excellence. Moreover, a BHAG can attract and retain top talent by offering a compelling vision and a sense of purpose. It acts as a magnet for individuals who are motivated by challenging and meaningful work. By embracing their BHAG, organizations can galvanize their employees and stakeholders, fostering a strong and unified culture where everyone is motivated and aligned towards achieving success.\n",
            "Human: What concept this book introduces?\n",
            "AI: This book introduces the concept of a Big Hairy Audacious Goal (BHAG).\n"
          ]
        }
      ],
      "source": [
        "print(conversation_buf.memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xucAqrt8RhLr",
        "outputId": "ce8e7999-a6a1-47ae-b579-858e6074be08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Good morning AI!\n",
            "AI: Good morning! How can I assist you today?\n",
            "Human: Can you please tell me your name AI?\n",
            "AI: Of course! My name is OpenAI. How can I assist you today?\n",
            "Human: This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\n",
            "AI: That sounds like a fascinating concept! The idea of a Big Hairy Audacious Goal (BHAG) was first introduced by James Collins and Jerry Porras in their book \"Built to Last: Successful Habits of Visionary Companies.\" They define a BHAG as a long-term goal that is both ambitious and inspiring, capable of energizing and aligning the efforts of employees and stakeholders. The purpose of a BHAG is to push an organization beyond its current capabilities and create a sense of purpose and direction. It's a powerful tool for motivating and guiding the actions of individuals and teams towards a common objective.\n",
            "Human: The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\n",
            "AI: Absolutely! The BHAG can indeed help an organization focus on a long-term goal by providing a clear and compelling vision for the future. It serves as a guiding star that helps leaders make strategic decisions and align resources towards achieving that goal. By setting a BHAG, organizations create a sense of urgency as they understand the importance of taking action and making progress towards their ambitious goal. This sense of urgency can drive innovation, collaboration, and a relentless pursuit of excellence. Moreover, a BHAG can attract and retain top talent by offering a compelling vision and a sense of purpose. It acts as a magnet for individuals who are motivated by challenging and meaningful work. By embracing their BHAG, organizations can galvanize their employees and stakeholders, fostering a strong and unified culture where everyone is motivated and aligned towards achieving success.\n",
            "Human: What concept this book introduces?\n",
            "AI: This book introduces the concept of a Big Hairy Audacious Goal (BHAG).\n"
          ]
        }
      ],
      "source": [
        "print(conversation_buf.memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6sYG_CwfTw_-"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "\n",
        "conversation_sum = ConversationChain(\n",
        "\tllm=llm,\n",
        "\tmemory=ConversationSummaryMemory(llm=llm)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEAdpB55UGWf",
        "outputId": "b9ec08cc-6685-4f44-9335-8b212a237444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2LM6V4ltUX7r",
        "outputId": "2e1db4a9-a5ad-4dd6-9ee0-fb12e074c3c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 283 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Of course! My name is OpenAI Assistant. How can I assist you today?'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# without count_tokens we'd call `conversation_sum(\"Can you please tell me your name AI?!\")`\n",
        "# but let's keep track of our tokens:\n",
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"Can you please tell me your name AI?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "duFZ3pA2U9ed",
        "outputId": "9ffe24ff-38d2-4df6-baba-6b84715b1386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 797 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'That sounds like an interesting concept! The idea of a Big Hairy Audacious Goal (BHAG) was first introduced by James Collins and Jerry Porras in their book \"Built to Last: Successful Habits of Visionary Companies.\" They describe a BHAG as a clear and compelling goal that is both audacious and visionary. It is a goal that goes beyond what seems possible in the present and requires significant effort and innovation to achieve. BHAGs are intended to inspire and motivate employees and stakeholders, helping to align their efforts towards a common purpose. They can be a powerful tool for driving organizational success and transformation. Is there anything specific you would like to know about BHAGs or how they can be applied in a business context?'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,\\\n",
        "     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "gxVtn7djVMjR",
        "outputId": "65fc8888-ec41-4801-ed8e-bcfe31f4a0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 1260 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Absolutely! Embracing a BHAG can indeed help an organization focus on a long-term goal and create a sense of urgency. By setting a bold and ambitious goal, companies can rally their employees and stakeholders around a shared vision, which in turn can foster a sense of purpose and motivation within the organization. This can lead to increased productivity, innovation, and overall success.\\n\\nAdditionally, having a BHAG can help attract and retain top talent. Talented individuals are often drawn to organizations that have a clear and compelling vision, as it provides them with a sense of purpose and the opportunity to work towards something meaningful. A BHAG can serve as a powerful tool for employee recruitment and retention, as it showcases the organization\\'s ambition and potential for growth.\\n\\n\"Built to Last\" by James Collins and Jerry Porras is indeed a great resource for learning more about BHAGs and how they can be applied in a business context. If you\\'d like, I can provide more specific examples or dive deeper into the concept of BHAGs. Just let me know how I can assist you further!'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0PT4deCIVbLN",
        "outputId": "68f03ba5-ebfc-45df-8704-7e25ba837c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 933 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm sorry, but I don't have enough information to answer your question. Could you please provide more context or clarify what book you are referring to?\""
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"What concept this book introduces?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Ytq1GQ5kVtFX",
        "outputId": "4a40b611-b44f-4124-a4ba-cf924cf71c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 1013 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I apologize for the confusion, but could you please provide more information or clarify which book you are referring to?'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"the book encourages readers of what?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tltc_6omWFWM",
        "outputId": "a9c6262e-adaf-45bb-fa7e-ea1166c3ec21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The human asks the AI for its name, and the AI responds that its name is OpenAI Assistant and asks how it can assist the human today. The human introduces the concept of a Big Hairy Audacious Goal (BHAG) and the AI explains that a BHAG is a clear and compelling goal that goes beyond what seems possible in the present and requires significant effort and innovation to achieve. The AI mentions that BHAGs inspire and motivate employees and stakeholders, helping to align their efforts towards a common purpose. The human adds that a BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The AI agrees, stating that embracing a BHAG can foster a sense of purpose and motivation within the organization, leading to increased productivity, innovation, and overall success. Having a BHAG can also attract and retain top talent, as talented individuals are drawn to organizations with a clear and compelling vision. The AI offers to provide more information or examples about BHAGs if needed. The human asks the AI about the concept introduced in a book. The AI apologizes and requests more information or clarification about the specific book being referred to. The human asks the AI what the book encourages readers of. The AI apologizes for the confusion and asks for more information or clarification about which book is being referred to.\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cHPAb_eeWOJk"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "conversation = ConversationChain(\n",
        "\tllm=llm,\n",
        "\tmemory=ConversationBufferWindowMemory(k=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "YUQU8mFJWWuE"
      },
      "outputs": [],
      "source": [
        "conversation_bufw = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferWindowMemory(k=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6qboNBLGWn8T",
        "outputId": "e3e61085-f73b-4a94-c931-aae6a75951a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 77 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Good morning! How can I assist you today?'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"Good morning AI!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "z1zdZS5YXM_q",
        "outputId": "ec6512dd-c4e6-4630-bd57-724e7d09ec27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 251 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Singularity refers to a hypothetical point in time when artificial intelligence surpasses human intelligence and becomes capable of self-improvement, leading to an exponential increase in technological advancements. It is often associated with the concept of superintelligence, where AI systems possess cognitive abilities far beyond those of humans.\\n\\nAs for when Singularity will occur, it is uncertain and highly debated among experts. Some believe it could happen within the next few decades, while others think it is still far off in the future. The timeline for Singularity depends on various factors, including advancements in AI research, computing power, and the development of new technologies.\\n\\nIt's important to note that Singularity is a speculative concept, and its exact nature and potential consequences are still subjects of intense discussion and study.\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"I want to know more about Singularity, what it is and when it is coming.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dWqoB9WyXZeH",
        "outputId": "46648587-ba6c-4f9f-c605-b337a793af6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 261 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sure, please let me know what specific information you would like to know about Singularity.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"What I want to know?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "lNk3aL7UX1-7",
        "outputId": "73963289-981a-4cf3-8c0b-1ecb1f56d7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 371 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The technology behind Singularity is quite complex and multidisciplinary. One of the key components is artificial intelligence (AI), which involves the development of algorithms and systems that can mimic human intelligence and perform tasks that typically require human intelligence, such as problem-solving, learning, and decision-making.\\n\\nAnother important technology is machine learning, a subset of AI that focuses on teaching computers to learn from and make predictions or decisions based on data. This involves training algorithms on large datasets and allowing them to automatically improve their performance over time.\\n\\nFurthermore, Singularity also relies on advancements in fields like robotics, natural language processing, computer vision, and data analytics. Robotics plays a crucial role in creating physical embodiments for AI systems, allowing them to interact with the physical world. Natural language processing enables machines to understand and generate human language, making communication with AI more natural and intuitive.\\n\\nComputer vision enables machines to perceive visual information and interpret images or videos. This is particularly important for tasks like object recognition or autonomous navigation. Data analytics helps in extracting meaningful insights from large amounts of data, which can be used to improve AI systems and make more informed decisions.\\n\\nOverall, the technology behind Singularity is a combination of AI, machine learning, robotics, natural language processing, computer vision, and data analytics. These technologies work together to create intelligent systems that can learn, adapt, and potentially surpass human capabilities.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"Give me the technology behind it\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5rE0rfOyYM1p",
        "outputId": "c2c4e55a-2ff6-4c7c-c620-26d9b9636ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 381 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm sorry, but I don't know what you want to know. Could you please provide more specific information or ask a specific question?\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"What I want to know?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Qz1F7JESYWnl"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mRv1DDS_Yggs"
      },
      "outputs": [],
      "source": [
        "conversation_sum_bufw = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationSummaryBufferMemory(\n",
        "        llm=llm,\n",
        "        max_token_limit=650\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aubvJyNYw9q",
        "outputId": "17d035ec-430b-4a0f-c191-9857aa240bd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum_bufw.memory.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri859_UuY0_Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
