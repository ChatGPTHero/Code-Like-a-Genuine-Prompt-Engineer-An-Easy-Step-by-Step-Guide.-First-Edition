{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lqcg7E5_UEr",
        "outputId": "e74a7fcd-05dc-4db8-e3f9-9196a0a81301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.353)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.7)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.4)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.75)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.4->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai tiktoken\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C-pPrC6wOayV"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import LLMChain, ConversationChain\n",
        "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
        "                                                  ConversationSummaryMemory,\n",
        "                                                  ConversationBufferWindowMemory,\n",
        "                                                  ConversationKGMemory)\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3ApzoD-YAARv"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCba-mbeA8Ia",
        "outputId": "49839b20-68b9-46ac-fb70-79b4c2978cc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: hi!\\nAI: what's up?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H8-ipRNLA_k3"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J71oaOOdBFiZ",
        "outputId": "ad7e99c5-d628-4a73-8d74-dd61435ec54f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': \"Human: hi!\\nAI: what's up?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8BApVU10BLxJ"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp4ddPa1Bh6K",
        "outputId": "77300e75-a640-459e-bbc4-e9cdaf0de8bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='hi!'),\n",
              "  AIMessage(content=\"what's up?\")]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "czoO1rn_EzCD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your api key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ccIIvNG7BuvP"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "# Notice that \"chat_history\" is present in the prompt template\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "New human question: {question}\n",
        "Response:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "# Notice that we need to align the `memory_key`\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlyjzIqcELDm",
        "outputId": "c626bf58-45df-4a25-f0db-b67ed5399f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
            "\n",
            "Previous conversation:\n",
            "\n",
            "\n",
            "New human question: hi\n",
            "Response:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': '',\n",
              " 'text': ' Hello! How are you doing today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pODaI1QOFVyW"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a nice chatbot having a conversation with a human.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzWjk5wBL2I1",
        "outputId": "8b4c0bba-bb5b-4c70-864a-a13f4d114579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
            "Human: hi\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='Hello! How can I help you today?')],\n",
              " 'text': 'Hello! How can I help you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fwtLjaH9L-Ib"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EAhlaeGvMpsb"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Z4Mdr9NImj",
        "outputId": "ef082226-218d-40ed-8c66-8da6271ed306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, Any],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        response = self.generate([inputs], run_manager=run_manager)\n",
            "        return self.create_outputs(response)[0]\n",
            "     def apply(\n",
            "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
            "    ) -> List[Dict[str, str]]:\n",
            "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
            "        callback_manager = CallbackManager.configure(\n",
            "            callbacks, self.callbacks, self.verbose\n",
            "        )\n",
            "        run_manager = callback_manager.on_chain_start(\n",
            "            dumpd(self),\n",
            "            {\"input_list\": input_list},\n",
            "        )\n",
            "        try:\n",
            "            response = self.generate(input_list, run_manager=run_manager)\n",
            "        except BaseException as e:\n",
            "            run_manager.on_chain_error(e)\n",
            "            raise e\n",
            "        outputs = self.create_outputs(response)\n",
            "        run_manager.on_chain_end({\"outputs\": outputs})\n",
            "        return outputs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWNt-YUIPnJz",
        "outputId": "f4db9881-9e62-4a7f-93d3-c96dd73c0484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, Any],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        response = self.generate([inputs], run_manager=run_manager)\n",
            "        return self.create_outputs(response)[0]\n",
            "     def apply(\n",
            "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
            "    ) -> List[Dict[str, str]]:\n",
            "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
            "        callback_manager = CallbackManager.configure(\n",
            "            callbacks, self.callbacks, self.verbose\n",
            "        )\n",
            "        run_manager = callback_manager.on_chain_start(\n",
            "            dumpd(self),\n",
            "            {\"input_list\": input_list},\n",
            "        )\n",
            "        try:\n",
            "            response = self.generate(input_list, run_manager=run_manager)\n",
            "        except BaseException as e:\n",
            "            run_manager.on_chain_error(e)\n",
            "            raise e\n",
            "        outputs = self.create_outputs(response)\n",
            "        run_manager.on_chain_end({\"outputs\": outputs})\n",
            "        return outputs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cXjkeMGDPuHq"
      },
      "outputs": [],
      "source": [
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rGYD8CWPz5o",
        "outputId": "ec18bc20-7417-477f-ee2b-62131606e258"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Good morning AI!',\n",
              " 'history': '',\n",
              " 'response': 'Good morning! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "conversation_buf(\"Good morning AI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQtq3bJWP3Dp",
        "outputId": "2943bb88-e515-4d62-ec59-9999efa74dac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Can you please tell me your name AI?',\n",
              " 'history': 'Human: Good morning AI!\\nAI: Good morning! How can I assist you today?',\n",
              " 'response': 'My name is OpenAI. I am an AI developed by OpenAI, a leading research organization in artificial intelligence. How may I help you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "conversation_buf(\"Can you please tell me your name AI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "61eaZyCrQbdN"
      },
      "outputs": [],
      "source": [
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "K03BBV5RQuXa",
        "outputId": "d95e50bb-eaeb-48db-daca-d3a2d0640eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 271 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That sounds interesting! The concept of a Big Hairy Audacious Goal (BHAG) was first introduced by Jim Collins and Jerry Porras in their book \"Built to Last: Successful Habits of Visionary Companies.\" A BHAG is a powerful tool that organizations can use to set a long-term vision and inspire their employees and stakeholders. It is meant to be ambitious and daring, pushing the boundaries of what seems possible. By having a BHAG, companies can rally their team around a common purpose and create a sense of shared mission.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,\\\n",
        "     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "8H_3KziPQ1nR",
        "outputId": "09e24729-66c7-4055-f54b-de0b9fa420cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 424 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Absolutely! Embracing a BHAG can indeed help an organization maintain focus on a long-term goal and create a sense of urgency. It provides a clear direction and purpose for everyone involved, which can be a powerful motivator. Additionally, having a BHAG can make an organization more attractive to top talent, as it signifies ambition and a commitment to achieving something truly meaningful. By aligning the efforts of employees and stakeholders towards a BHAG, organizations can galvanize their teams and increase their chances of success.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fturnCyNREeJ",
        "outputId": "f4883110-4796-444b-f65a-8be52d3fab78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 451 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This book introduces the concept of a Big Hairy Audacious Goal (BHAG).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"What concept this book introduces?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIe6VI0DRVW4",
        "outputId": "e06a200b-0e34-4072-c3c5-68f94d8a966c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Good morning AI!\n",
            "AI: Good morning! How can I assist you today?\n",
            "Human: Can you please tell me your name AI?\n",
            "AI: My name is OpenAI. I am an AI developed by OpenAI, a leading research organization in artificial intelligence. How may I help you today?\n",
            "Human: This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\n",
            "AI: That sounds interesting! The concept of a Big Hairy Audacious Goal (BHAG) was first introduced by Jim Collins and Jerry Porras in their book \"Built to Last: Successful Habits of Visionary Companies.\" A BHAG is a powerful tool that organizations can use to set a long-term vision and inspire their employees and stakeholders. It is meant to be ambitious and daring, pushing the boundaries of what seems possible. By having a BHAG, companies can rally their team around a common purpose and create a sense of shared mission.\n",
            "Human: The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\n",
            "AI: Absolutely! Embracing a BHAG can indeed help an organization maintain focus on a long-term goal and create a sense of urgency. It provides a clear direction and purpose for everyone involved, which can be a powerful motivator. Additionally, having a BHAG can make an organization more attractive to top talent, as it signifies ambition and a commitment to achieving something truly meaningful. By aligning the efforts of employees and stakeholders towards a BHAG, organizations can galvanize their teams and increase their chances of success.\n",
            "Human: What concept this book introduces?\n",
            "AI: This book introduces the concept of a Big Hairy Audacious Goal (BHAG).\n"
          ]
        }
      ],
      "source": [
        "print(conversation_buf.memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6sYG_CwfTw_-"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "\n",
        "conversation_sum = ConversationChain(\n",
        "\tllm=llm,\n",
        "\tmemory=ConversationSummaryMemory(llm=llm)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEAdpB55UGWf",
        "outputId": "6d62bc06-56a9-429a-bfe3-8c87fee4c380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2LM6V4ltUX7r",
        "outputId": "f59caff1-e52b-47e6-a6ed-5cc50ec05337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 344 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI developed by OpenAI, known as GPT-3. My purpose is to assist and engage in conversations with users like yourself. How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# without count_tokens we'd call `conversation_sum(\"Can you please tell me your name AI?!\")`\n",
        "# but let's keep track of our tokens:\n",
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"Can you please tell me your name AI?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "duFZ3pA2U9ed",
        "outputId": "becf34a8-9507-4d39-e4b9-20d6f2e9362b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 894 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That sounds interesting! The concept of a Big Hairy Audacious Goal (BHAG) was actually popularized by Jim Collins and Jerry Porras in their book \"Built to Last: Successful Habits of Visionary Companies.\" They define a BHAG as a long-term, ambitious, and inspiring goal that can rally people together and drive organizational success. It\\'s meant to be a clear and compelling vision that stretches and challenges the organization, while also being achievable. BHAGs are often used by companies to create a sense of purpose and direction, and they can be a powerful tool for motivating employees and aligning their efforts towards a common objective. Is there anything specific you would like to know about BHAGs or how they can be implemented?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,\\\n",
        "     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "gxVtn7djVMjR",
        "outputId": "ae304268-6b45-41a3-cdc5-50471861ab0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 1262 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That\\'s absolutely right! Embracing a BHAG can indeed help an organization focus on a long-term goal and create a sense of urgency. It provides a clear direction and purpose for the organization, which can motivate employees and align their efforts. By setting a big, ambitious goal, companies can attract and retain top talent who are excited to be part of something meaningful and impactful.\\n\\nThe book \"Built to Last: Successful Habits of Visionary Companies\" by Jim Collins and Jerry Porras explores the concept of BHAGs in depth. It showcases how successful companies have used BHAGs to drive their success and sustain their competitive advantage over time. The authors emphasize the importance of having an audacious goal that inspires and stretches the organization, rather than settling for mediocrity.\\n\\nIf you have any more questions about BHAGs or their implementation, feel free to ask!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "0PT4deCIVbLN",
        "outputId": "86e9a3cd-01ad-4731-ffe7-a02aa9d6c249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 910 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The book \"Built to Last: Successful Habits of Visionary Companies\" introduces the concept of BHAGs, which stands for \"Big Hairy Audacious Goals.\" BHAGs are long-term, ambitious goals that can motivate and align employees. They are meant to be challenging and exciting, and they can help organizations focus, create a sense of urgency, and attract top talent. The book explores how successful companies have used BHAGs to drive their success and provides insights into the habits and practices of these visionary companies.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"What concept this book introduces?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Ytq1GQ5kVtFX",
        "outputId": "0b53e1a1-940d-4a43-d695-3f8da233b67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 702 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The book \"Built to Last: Successful Habits of Visionary Companies\" encourages readers to adopt and implement the concept of BHAGs, which stands for \"Big Hairy Audacious Goals.\" BHAGs are long-term, ambitious goals that can motivate and align employees. The book emphasizes the importance of setting challenging and exciting goals that can help organizations focus, create a sense of urgency, and attract top talent.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"the book encourages readers of what?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tltc_6omWFWM",
        "outputId": "6613db99-2f02-43d5-faa8-aebcd14e1cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AI explains that the book \"Built to Last: Successful Habits of Visionary Companies\" encourages readers to adopt and implement the concept of BHAGs, which are long-term, ambitious goals that can motivate and align employees. BHAGs are meant to be challenging and exciting, and they can help organizations focus, create a sense of urgency, and attract top talent. The book explores how successful companies have used BHAGs to drive their success and provides insights into the habits and practices of these visionary companies.\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cHPAb_eeWOJk"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YUQU8mFJWWuE"
      },
      "outputs": [],
      "source": [
        "conversation_bufw = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferWindowMemory(k=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6qboNBLGWn8T",
        "outputId": "d5595777-b60a-4093-dd0e-48abdab5989c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 77 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Good morning! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"Good morning AI!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "z1zdZS5YXM_q",
        "outputId": "3aba2574-7721-4157-a0e0-e72b0b712af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 301 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Singularity refers to a hypothetical future point in time when technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization. It is often associated with the emergence of superintelligent machines that surpass human intelligence in every aspect. The concept was popularized by mathematician and computer scientist Vernor Vinge in the 1990s and further explored by futurist Ray Kurzweil.\\n\\nAs for when Singularity might occur, it is difficult to predict with certainty. Some experts believe it could happen within the next few decades, while others argue that it is still far off in the future. The development and arrival of superintelligent AI systems are dependent on various factors, including advancements in technology, algorithms, and computational power.\\n\\nIt is worth noting that the concept of Singularity is highly debated, and there are different opinions regarding its feasibility and potential impact. Some see it as a potential source of great progress and positive change, while others express concerns about potential risks and ethical implications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"I want to know more about Singularity, what it is and when it is coming.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dWqoB9WyXZeH",
        "outputId": "6a62feda-2e2c-46b2-852b-06f5c73e9640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 323 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Please let me know what specific information you would like to know about Singularity, and I will do my best to provide you with the requested details.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"What I want to know?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "lNk3aL7UX1-7",
        "outputId": "571bcb5d-a411-42b8-b708-7db91c8f1b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 449 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The concept of Singularity refers to a hypothetical point in the future where artificial intelligence (AI) surpasses human intelligence, leading to rapid technological advancements that are beyond our current comprehension. While the idea of Singularity encompasses various aspects, there are several key technologies that are often associated with its potential realization.\\n\\nOne of the fundamental technologies behind Singularity is artificial general intelligence (AGI). AGI refers to highly autonomous systems that possess the ability to outperform humans in most economically valuable work. It involves creating AI systems that can understand, learn, and apply knowledge across a wide range of tasks and domains. AGI is an essential component for achieving Singularity since it requires machines that are capable of surpassing human intellectual capabilities.\\n\\nAnother technology that plays a crucial role in Singularity is machine learning. Machine learning involves developing algorithms and models that enable computers to learn and improve from data without being explicitly programmed. This field has seen tremendous advancements in recent years and has contributed to various AI applications, such as image recognition, natural language processing, and recommendation systems.\\n\\nIn addition to AGI and machine learning, other technologies that contribute to the realization of Singularity include robotics, nanotechnology, and advanced computing. Robotics involves designing and constructing physical machines that can interact with and manipulate the physical world. Nanotechnology focuses on manipulating matter at the atomic and molecular scale, potentially enabling the creation of advanced materials and devices. Advanced computing, including quantum computing, can provide the necessary computational power to process vast amounts of data and perform complex calculations.\\n\\nIt is important to note that the concept of Singularity is still highly debated and speculative. While these technologies contribute to its potential realization, the timeline and feasibility of Singularity remain uncertain.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"Give me the technology behind it\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "5rE0rfOyYM1p",
        "outputId": "4401adf3-d0d8-4f08-8191-f33b8995fb3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 468 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I apologize, but I do not have the ability to know what specific information you want to know without you telling me. Please provide me with your question or topic of interest, and I will do my best to provide you with the relevant information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"What I want to know?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Qz1F7JESYWnl"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mRv1DDS_Yggs"
      },
      "outputs": [],
      "source": [
        "conversation_sum_bufw = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationSummaryBufferMemory(\n",
        "        llm=llm,\n",
        "        max_token_limit=650\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aubvJyNYw9q",
        "outputId": "32e7c133-aff3-4434-a8a5-eef3db7bf2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum_bufw.memory.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri859_UuY0_Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}