{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lqcg7E5_UEr",
        "outputId": "be249e7b-78e2-4546-c89e-7ffb34b0e25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.353)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.7)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.4)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.75)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.4->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain openai tiktoken\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C-pPrC6wOayV"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import LLMChain, ConversationChain\n",
        "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
        "                                                  ConversationSummaryMemory,\n",
        "                                                  ConversationBufferWindowMemory,\n",
        "                                                  ConversationKGMemory)\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3ApzoD-YAARv"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCba-mbeA8Ia",
        "outputId": "1062442c-ba7c-445c-af15-00de2fdd3b63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: hi!\\nAI: what's up?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H8-ipRNLA_k3"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J71oaOOdBFiZ",
        "outputId": "586802a7-ae19-473d-94e9-e1e1bb31738b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': \"Human: hi!\\nAI: what's up?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8BApVU10BLxJ"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp4ddPa1Bh6K",
        "outputId": "f34e5897-8c79-4635-d414-311a6092db1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='hi!'),\n",
              "  AIMessage(content=\"what's up?\")]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "czoO1rn_EzCD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your api key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ccIIvNG7BuvP"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "# Notice that \"chat_history\" is present in the prompt template\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "New human question: {question}\n",
        "Response:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "# Notice that we need to align the `memory_key`\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlyjzIqcELDm",
        "outputId": "367fe9c1-e7e2-4d80-a6dd-b044cf414fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
            "\n",
            "Previous conversation:\n",
            "\n",
            "\n",
            "New human question: hi\n",
            "Response:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': '',\n",
              " 'text': ' Hello! How are you doing today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pODaI1QOFVyW"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a nice chatbot having a conversation with a human.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzWjk5wBL2I1",
        "outputId": "e6b87f04-6fde-4ac1-994b-f6c37150eac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
            "Human: hi\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='Hello! How can I assist you today?')],\n",
              " 'text': 'Hello! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fwtLjaH9L-Ib"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EAhlaeGvMpsb"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Z4Mdr9NImj",
        "outputId": "8ed855d2-1d84-49f8-ce72-65d356e1b6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, Any],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        response = self.generate([inputs], run_manager=run_manager)\n",
            "        return self.create_outputs(response)[0]\n",
            "     def apply(\n",
            "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
            "    ) -> List[Dict[str, str]]:\n",
            "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
            "        callback_manager = CallbackManager.configure(\n",
            "            callbacks, self.callbacks, self.verbose\n",
            "        )\n",
            "        run_manager = callback_manager.on_chain_start(\n",
            "            dumpd(self),\n",
            "            {\"input_list\": input_list},\n",
            "        )\n",
            "        try:\n",
            "            response = self.generate(input_list, run_manager=run_manager)\n",
            "        except BaseException as e:\n",
            "            run_manager.on_chain_error(e)\n",
            "            raise e\n",
            "        outputs = self.create_outputs(response)\n",
            "        run_manager.on_chain_end({\"outputs\": outputs})\n",
            "        return outputs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWNt-YUIPnJz",
        "outputId": "ec81a6ba-f0cf-426b-a741-c2f408c620cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, Any],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        response = self.generate([inputs], run_manager=run_manager)\n",
            "        return self.create_outputs(response)[0]\n",
            "     def apply(\n",
            "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
            "    ) -> List[Dict[str, str]]:\n",
            "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
            "        callback_manager = CallbackManager.configure(\n",
            "            callbacks, self.callbacks, self.verbose\n",
            "        )\n",
            "        run_manager = callback_manager.on_chain_start(\n",
            "            dumpd(self),\n",
            "            {\"input_list\": input_list},\n",
            "        )\n",
            "        try:\n",
            "            response = self.generate(input_list, run_manager=run_manager)\n",
            "        except BaseException as e:\n",
            "            run_manager.on_chain_error(e)\n",
            "            raise e\n",
            "        outputs = self.create_outputs(response)\n",
            "        run_manager.on_chain_end({\"outputs\": outputs})\n",
            "        return outputs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cXjkeMGDPuHq"
      },
      "outputs": [],
      "source": [
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rGYD8CWPz5o",
        "outputId": "4726e186-f886-4ea5-e064-35e9d03ff054"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Good morning AI!',\n",
              " 'history': '',\n",
              " 'response': 'Good morning! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "conversation_buf(\"Good morning AI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQtq3bJWP3Dp",
        "outputId": "030c6455-720c-45de-fa95-5ccd517c25bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Can you please tell me your name AI?',\n",
              " 'history': 'Human: Good morning AI!\\nAI: Good morning! How can I assist you today?',\n",
              " 'response': 'Of course! My name is OpenAI, or GPT-3 for short. How can I help you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "conversation_buf(\"Can you please tell me your name AI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "61eaZyCrQbdN"
      },
      "outputs": [],
      "source": [
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "K03BBV5RQuXa",
        "outputId": "d280510a-c71e-443c-f8bb-0f437f09a8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 181 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That sounds interesting! Would you like me to provide more information about the concept of BHAGs or any specific examples?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,\\\n",
        "     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "8H_3KziPQ1nR",
        "outputId": "c5b078a0-b843-4c11-e3da-e0074a5db4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 354 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's great to hear! BHAGs can indeed be a powerful tool for organizations. By setting a long-term and ambitious goal, companies can create a sense of urgency and inspire their employees to work towards a common objective. This can lead to increased motivation and productivity within the organization. BHAGs can also attract and retain top talent as they provide a clear and compelling vision for the future. It's wonderful that the book encourages readers to embrace their BHAGs and use them to galvanize their organizations towards success. Is there anything else you would like to know about BHAGs or any other topic?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fturnCyNREeJ",
        "outputId": "77c284ad-b785-4f85-86bc-8a6a5641c51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 381 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This book introduces the concept of a Big Hairy Audacious Goal (BHAG).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"What concept this book introduces?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIe6VI0DRVW4",
        "outputId": "15c9bb98-bfac-42ba-d9c0-6e5265107491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Good morning AI!\n",
            "AI: Good morning! How can I assist you today?\n",
            "Human: Can you please tell me your name AI?\n",
            "AI: Of course! My name is OpenAI, or GPT-3 for short. How can I help you today?\n",
            "Human: This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\n",
            "AI: That sounds interesting! Would you like me to provide more information about the concept of BHAGs or any specific examples?\n",
            "Human: The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\n",
            "AI: That's great to hear! BHAGs can indeed be a powerful tool for organizations. By setting a long-term and ambitious goal, companies can create a sense of urgency and inspire their employees to work towards a common objective. This can lead to increased motivation and productivity within the organization. BHAGs can also attract and retain top talent as they provide a clear and compelling vision for the future. It's wonderful that the book encourages readers to embrace their BHAGs and use them to galvanize their organizations towards success. Is there anything else you would like to know about BHAGs or any other topic?\n",
            "Human: What concept this book introduces?\n",
            "AI: This book introduces the concept of a Big Hairy Audacious Goal (BHAG).\n"
          ]
        }
      ],
      "source": [
        "print(conversation_buf.memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xucAqrt8RhLr",
        "outputId": "a09a52f0-99a6-42f2-c8a1-1fe3baf8b154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Good morning AI!\n",
            "AI: Good morning! How can I assist you today?\n",
            "Human: Can you please tell me your name AI?\n",
            "AI: Of course! My name is OpenAI, or GPT-3 for short. How can I help you today?\n",
            "Human: This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\n",
            "AI: That sounds interesting! Would you like me to provide more information about the concept of BHAGs or any specific examples?\n",
            "Human: The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\n",
            "AI: That's great to hear! BHAGs can indeed be a powerful tool for organizations. By setting a long-term and ambitious goal, companies can create a sense of urgency and inspire their employees to work towards a common objective. This can lead to increased motivation and productivity within the organization. BHAGs can also attract and retain top talent as they provide a clear and compelling vision for the future. It's wonderful that the book encourages readers to embrace their BHAGs and use them to galvanize their organizations towards success. Is there anything else you would like to know about BHAGs or any other topic?\n",
            "Human: What concept this book introduces?\n",
            "AI: This book introduces the concept of a Big Hairy Audacious Goal (BHAG).\n"
          ]
        }
      ],
      "source": [
        "print(conversation_buf.memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6sYG_CwfTw_-"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "\n",
        "conversation_sum = ConversationChain(\n",
        "\tllm=llm,\n",
        "\tmemory=ConversationSummaryMemory(llm=llm)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEAdpB55UGWf",
        "outputId": "6de7389c-4374-4d71-f86e-797ec50bf12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2LM6V4ltUX7r",
        "outputId": "1d14a2a1-6196-40c4-b6e2-a0e4fee5357f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 288 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Of course! My name is OpenAI.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# without count_tokens we'd call `conversation_sum(\"Can you please tell me your name AI?!\")`\n",
        "# but let's keep track of our tokens:\n",
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"Can you please tell me your name AI?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "duFZ3pA2U9ed",
        "outputId": "24cd3ba4-304f-4c06-bb8e-7310b3b962f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 663 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That sounds like a fascinating concept! A Big Hairy Audacious Goal (BHAG) is a powerful tool for organizations to set long-term objectives that are ambitious and inspiring. By having a BHAG, employees and stakeholders can be motivated and aligned towards a common purpose, driving innovation and progress. It's a great way to keep everyone focused and working towards a shared vision.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"This book introduces the concept of a Big Hairy Audacious Goal (BHAG), a long-term, ambitious,\\\n",
        "     and inspiring goal that can motivate and align the efforts of employees and stakeholders.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "gxVtn7djVMjR",
        "outputId": "26ad8d71-de51-4f9d-9ff9-dd79681d328c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 886 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That sounds like a great strategy! By setting a Big Hairy Audacious Goal (BHAG), organizations can indeed create a sense of urgency and align their employees towards a common objective. It can also help attract and retain top talent, as people are often motivated by the opportunity to work towards ambitious and inspiring goals. The book you mentioned seems to provide valuable insights on how to embrace BHAGs and drive success within organizations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"The BHAG can help an organization focus on a long-term goal, create a sense of urgency, and attract and retain top talent. The book encourages readers to embrace their BHAGs and galvanize their organizations towards success.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0PT4deCIVbLN",
        "outputId": "ea79f6cb-b2fa-440f-cd75-2e92d761f575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 755 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The book introduces the concept of a Big Hairy Audacious Goal (BHAG).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"What concept this book introduces?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Ytq1GQ5kVtFX",
        "outputId": "c95e2989-693c-41f5-9fb5-3143ce95fc6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 882 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The book encourages readers to embrace the concept of a Big Hairy Audacious Goal (BHAG) and provides insights on how to drive success within organizations by setting and pursuing these ambitious goals.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"the book encourages readers of what?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tltc_6omWFWM",
        "outputId": "0d3a4edf-c337-4b62-9539-7fa1fec6138c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human asks the AI for its name, and the AI responds that its name is OpenAI. The human introduces the concept of a Big Hairy Audacious Goal (BHAG), explaining its benefits. The AI finds the concept fascinating and explains further how a BHAG can help organizations. The human mentions that embracing BHAGs can create a sense of urgency, attract top talent, and drive success. The AI agrees and adds that setting a BHAG can align employees, motivate them, and provide a shared vision. The AI also acknowledges the value of the book mentioned in providing insights on embracing BHAGs and driving success within organizations. The human asks what concept this book introduces, and the AI responds that the book encourages readers to embrace the concept of a Big Hairy Audacious Goal (BHAG) and provides insights on how to drive success within organizations by setting and pursuing these ambitious goals.\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cHPAb_eeWOJk"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "conversation = ConversationChain(\n",
        "\tllm=llm,\n",
        "\tmemory=ConversationBufferWindowMemory(k=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YUQU8mFJWWuE"
      },
      "outputs": [],
      "source": [
        "conversation_bufw = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferWindowMemory(k=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6qboNBLGWn8T",
        "outputId": "9700c1cc-94d7-44a6-f56e-e3a4a28168e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 77 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Good morning! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"Good morning AI!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "z1zdZS5YXM_q",
        "outputId": "3aba2574-7721-4157-a0e0-e72b0b712af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 301 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Singularity refers to a hypothetical future point in time when technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization. It is often associated with the emergence of superintelligent machines that surpass human intelligence in every aspect. The concept was popularized by mathematician and computer scientist Vernor Vinge in the 1990s and further explored by futurist Ray Kurzweil.\\n\\nAs for when Singularity might occur, it is difficult to predict with certainty. Some experts believe it could happen within the next few decades, while others argue that it is still far off in the future. The development and arrival of superintelligent AI systems are dependent on various factors, including advancements in technology, algorithms, and computational power.\\n\\nIt is worth noting that the concept of Singularity is highly debated, and there are different opinions regarding its feasibility and potential impact. Some see it as a potential source of great progress and positive change, while others express concerns about potential risks and ethical implications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"I want to know more about Singularity, what it is and when it is coming.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dWqoB9WyXZeH",
        "outputId": "6a62feda-2e2c-46b2-852b-06f5c73e9640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 323 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Please let me know what specific information you would like to know about Singularity, and I will do my best to provide you with the requested details.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"What I want to know?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "lNk3aL7UX1-7",
        "outputId": "571bcb5d-a411-42b8-b708-7db91c8f1b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 449 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The concept of Singularity refers to a hypothetical point in the future where artificial intelligence (AI) surpasses human intelligence, leading to rapid technological advancements that are beyond our current comprehension. While the idea of Singularity encompasses various aspects, there are several key technologies that are often associated with its potential realization.\\n\\nOne of the fundamental technologies behind Singularity is artificial general intelligence (AGI). AGI refers to highly autonomous systems that possess the ability to outperform humans in most economically valuable work. It involves creating AI systems that can understand, learn, and apply knowledge across a wide range of tasks and domains. AGI is an essential component for achieving Singularity since it requires machines that are capable of surpassing human intellectual capabilities.\\n\\nAnother technology that plays a crucial role in Singularity is machine learning. Machine learning involves developing algorithms and models that enable computers to learn and improve from data without being explicitly programmed. This field has seen tremendous advancements in recent years and has contributed to various AI applications, such as image recognition, natural language processing, and recommendation systems.\\n\\nIn addition to AGI and machine learning, other technologies that contribute to the realization of Singularity include robotics, nanotechnology, and advanced computing. Robotics involves designing and constructing physical machines that can interact with and manipulate the physical world. Nanotechnology focuses on manipulating matter at the atomic and molecular scale, potentially enabling the creation of advanced materials and devices. Advanced computing, including quantum computing, can provide the necessary computational power to process vast amounts of data and perform complex calculations.\\n\\nIt is important to note that the concept of Singularity is still highly debated and speculative. While these technologies contribute to its potential realization, the timeline and feasibility of Singularity remain uncertain.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"Give me the technology behind it\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "5rE0rfOyYM1p",
        "outputId": "4401adf3-d0d8-4f08-8191-f33b8995fb3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 468 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I apologize, but I do not have the ability to know what specific information you want to know without you telling me. Please provide me with your question or topic of interest, and I will do my best to provide you with the relevant information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"What I want to know?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Qz1F7JESYWnl"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mRv1DDS_Yggs"
      },
      "outputs": [],
      "source": [
        "conversation_sum_bufw = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationSummaryBufferMemory(\n",
        "        llm=llm,\n",
        "        max_token_limit=650\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aubvJyNYw9q",
        "outputId": "32e7c133-aff3-4434-a8a5-eef3db7bf2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum_bufw.memory.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri859_UuY0_Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}